Lecture Notes
========================================================

Data
----

File            | Source
--------------- | ------
samsungData.rda | https://spark-public.s3.amazonaws.com/dataanalysis/samsungData.rda
hunger.csv:     | https://spark-public.s3.amazonaws.com/dataanalysis/hunger.csv
income.csv      | https://spark-public.s3.amazonaws.com/dataanalysis/income.csv
movies.csv      | https://spark-public.s3.amazonaws.com/dataanalysis/movies

Download data files
-------------------

```{r}
# download.file("https://spark-public.s3.amazonaws.com/dataanalysis/samsungData.rda", "./data/samsungData.rda", method="curl")
# download.file("https://spark-public.s3.amazonaws.com/dataanalysis/hunger.csv", "./data/hunger.csv", method="curl")
# download.file("https://spark-public.s3.amazonaws.com/dataanalysis/income.csv", "./data/income.csv", method="curl")
#download.file("https://spark-public.s3.amazonaws.com/dataanalysis/movies.txt", "./data/movies.txt", method="curl")
```

Read data files
---------------

```{r}
load("./data/samsungData.rda")
hunger <- read.csv("./data/hunger.csv")
income <- read.csv("./data/income.csv")
movies <- read.table("./data/movies.txt", sep="\t")
```

Clustering Example
------------------

Accelerator data from samsung phones collected while performing different activities.

Try and predict what kind of activity a subject is involved in.


```{r}
names(samsungData)[1:12]
table(samsungData$activity)
```

Average acceleration for first subject.

i.e. samsungData[samsungData$subject==1,1] is the mean acceleration for the first subject in x-axis
i.e. samsungData[samsungData$subject==1,2] is the mean acceleration for the first subject in y-axis

names(samsungData)[1:3] # first three columns are mean acc x,y,z

```{r}
par(mfrow=c(1,2))
numericActivity <- as.numeric(as.factor(samsungData$activity))[samsungData$subject==1]
plot(samsungData[samsungData$subject==1,1],pch=19,col=numericActivity,ylab=names(samsungData)[1]) 
plot(samsungData[samsungData$subject==1,2],pch=19,col=numericActivity,ylab=names(samsungData)[2]) 
legend(150,-0.1,legend=unique(samsungData$activity),col=unique(numericActivity),pch=19)
```

Clustering base just on average acceleration.

activity does not cluster acceleration very well:


```{r}
source("http://dl.dropbox.com/u/7710864/courseraPublic/myplclust.R") 
distanceMatrix <- dist(samsungData[samsungData$subject==1,1:3])
hclustering <- hclust(distanceMatrix) 
myplclust(hclustering,lab.col=numericActivity)
```

Plotting max acceleration for the first subject.

Get better clustering based on max acceleration

```{r}
par(mfrow=c(1,2)) 
plot(samsungData[samsungData$subject==1,10],pch=19,col=numericActivity,ylab=names(samsungData)[10]) 
plot(samsungData[samsungData$subject==1,11],pch=19,col=numericActivity,ylab=names(samsungData)[11])
```

Clustering based on maximum acceleration

walking down cluster is now well defined

```{r}
source("http://dl.dropbox.com/u/7710864/courseraPublic/myplclust.R") 
distanceMatrix <- dist(samsungData[samsungData$subject==1,10:12]) 
hclustering <- hclust(distanceMatrix) 
myplclust(hclustering,lab.col=numericActivity)
```

Singular value decomposition

u  = left singular vector

second singular vector (avg of multiple patterns)


```{r}
svd1 = svd(scale(samsungData[samsungData$subject==1,-c(562,563)])) 
par(mfrow=c(1,2))
plot(svd1$u[,1],col=numericActivity,pch=19) 
plot(svd1$u[,2],col=numericActivity,pch=19)
```

Find maximum contributor

look at second right singular vector
gives weights that each variable contributes to variation in left singular vector

```{r}
plot(svd1$v[,2],pch=19)
```

New clustering with maximum contributer

separates out light blue, blue and pink


```{r}
maxContrib <- which.max(svd1$v[,2])
distanceMatrix <- dist(samsungData[samsungData$subject==1,c(10:12,maxContrib)]) 
hclustering <- hclust(distanceMatrix) 
myplclust(hclustering,lab.col=numericActivity)
```

This is the variable that separates out walking from walking up.

```{r}
names(samsungData)[maxContrib]
```

K-means clustering (nstart = 1, first try)

```{r}
kClust <- kmeans(samsungData[samsungData$subject==1,-c(562,563)],centers=6) 
table(kClust$cluster,samsungData$activity[samsungData$subject==1])
```

second try, same parameters. shows clusters still not clear


```{r}
kClust <- kmeans(samsungData[samsungData$subject==1,-c(562,563)],centers=6,nstart=1) 
table(kClust$cluster,samsungData$activity[samsungData$subject==1])
```

increase attempts to 100, to get better clusters

```{r}
kClust <- kmeans(samsungData[samsungData$subject==1,-c(562,563)],centers=6,nstart=100) 
table(kClust$cluster,samsungData$activity[samsungData$subject==1])
```

second try with 100 restarts

```{r}
kClust <- kmeans(samsungData[samsungData$subject==1,-c(562,563)],centers=6,nstart=100) 
table(kClust$cluster,samsungData$activity[samsungData$subject==1])
```

Cluster 1 Variable centers

what do the k-means clusters mean?

Laying:

```{r}
plot(kClust$center[1,1:10],pch=19,ylab="Cluster Center",xlab="")
```

Walking:

```{r}
plot(kClust$center[6,1:10],pch=19,ylab="Cluster Center",xlab="")
```

Basic Least Squares
-------------------

* describe distribution of variables
* describe relationship between variables
* make inferences about distribution and relationships

can you predict the height of the child from the average of the heights of the parents?

Distribution of data:

```{r}
library(UsingR); 
data(galton) 
par(mfrow=c(1,2)) 
hist(galton$child,col="blue",breaks=100) 
hist(galton$parent,col="blue",breaks=100)
```

Distribution of child heights:

```{r}
hist(galton$child,col="blue",breaks=100)
```

average child height 

average summarises evenly distributed data

```{r}
hist(galton$child,col="blue",breaks=100)
meanChild <- mean(galton$child) 
lines(rep(meanChild,100),seq(0,150,length=100),col="red",lwd=5)
```

child vs average parent

```{r}
plot(galton$parent,galton$child,pch=19,col="blue")
```

jittered plot

shows stacked points by adding some random noise to points

```{r}
set.seed(1234) 
plot(jitter(galton$parent,factor=2),jitter(galton$child,factor=2),pch=19,col="blue")
```

average parent = 65'' tall

```{r}
plot(galton$parent,galton$child,pch=19,col="blue")
near65 <- galton[abs(galton$parent - 65)<1, ] 
points(near65$parent,near65$child,pch=19,col="red") 
lines(seq(64,66,length=100),rep(mean(near65$child),100),col="red",lwd=4)
```

average parent = 75'' tall

```{r}
plot(galton$parent,galton$child,pch=19,col="blue")
near71 <- galton[abs(galton$parent - 71)<1, ] 
points(near71$parent,near71$child,pch=19,col="red") 
lines(seq(70,72,length=100),rep(mean(near71$child),100),col="red",lwd=4)
```

fit a line to summarize the data using the lm linear model function

```{r}
plot(galton$parent,galton$child,pch=19,col="blue") 
lm1 <- lm(galton$child ~ galton$parent) 
lines(galton$parent,lm1$fitted,col="red",lwd=3)
```


Not good to just plot this (does not account for error terms)

plot(galton$parent,galton$child,pch=19,col="blue") 
lines(galton$parent, 26 + 0.646*galton$parent)

plot(galton$parent,galton$child,pch=19,col="blue") 
lines(galton$parent,lm1$fitted,col="red",lwd=3)

residuals are differences between best fit and the points

```{r}
par(mfrow=c(1,2)) 
plot(galton$parent,galton$child,pch=19,col="blue") 
lines(galton$parent,lm1$fitted,col="red",lwd=3) 
plot(galton$parent,lm1$residuals,col="blue",pch=19) 
abline(c(0,0),col="red",lwd=3)
```

Inference Basics
----------------

Using galton child and parent height data:

use linear model to fit line

```{r}
library(UsingR); data(galton); 
plot(galton$parent,galton$child,pch=19,col="blue") 
lm1 <- lm(galton$child ~ galton$parent) 
lines(galton$parent,lm1$fitted,col="red",lwd=3)
```

The slope and intercept of the line given by:


```{r}
lm1
```

Generate a large dataset that follows the galton data

```{r}
newGalton <- data.frame(parent=rep(NA,1e6),child=rep(NA,1e6))
newGalton$parent <- rnorm(1e6,mean=mean(galton$parent),sd=sd(galton$parent))
newGalton$child <- lm1$coeff[1] + lm1$coeff[2]*newGalton$parent + rnorm(1e6,sd=sd(lm1$residuals)) 
smoothScatter(newGalton$parent,newGalton$child)
abline(lm1,col="red",lwd=3)
```

Sample the data and calculate the best fit

```{r}
set.seed(134325); sampleGalton1 <- newGalton[sample(1:1e6,size=50,replace=F),] 
sampleLm1 <- lm(sampleGalton1$child ~ sampleGalton1$parent) 
plot(sampleGalton1$parent,sampleGalton1$child,pch=19,col="blue") 
lines(sampleGalton1$parent,sampleLm1$fitted,lwd=3,lty=2) 
abline(lm1,col="red",lwd=3)
```

another sample

```{r}
sampleGalton2 <- newGalton[sample(1:1e6,size=50,replace=F),] 
sampleLm2 <- lm(sampleGalton2$child ~ sampleGalton2$parent) 
plot(sampleGalton2$parent,sampleGalton2$child,pch=19,col="blue") 
lines(sampleGalton2$parent,sampleLm2$fitted,lwd=3,lty=2) 
abline(lm1,col="red",lwd=3)
```

another sample

```{r}
sampleGalton3 <- newGalton[sample(1:1e6,size=50,replace=F),] 
sampleLm3 <- lm(sampleGalton3$child ~ sampleGalton3$parent) 
plot(sampleGalton3$parent,sampleGalton3$child,pch=19,col="blue") 
lines(sampleGalton3$parent,sampleLm3$fitted,lwd=3,lty=2) 
abline(lm1,col="red",lwd=3)
```

many samples

```{r}
sampleLm <- vector(100,mode="list") 
for(i in 1:100){
  sampleGalton <- newGalton[sample(1:1e6,size=50,replace=F),]
  sampleLm[[i]] <- lm(sampleGalton$child ~ sampleGalton$parent) 
}
```

plot against many samples

```{r}
smoothScatter(newGalton$parent,newGalton$child) 
for(i in 1:100){abline(sampleLm[[i]],lwd=3,lty=2)} 
abline(lm1,col="red",lwd=3)
```

Histograms show data is centred on the slope and intercept used to generate the data

```{r}
par(mfrow=c(1,2)) 
hist(sapply(sampleLm,function(x){coef(x)[1]}),col="blue",xlab="Intercept",main="") 
hist(sapply(sampleLm,function(x){coef(x)[2]}),col="blue",xlab="Slope",main="")
```

Central limit theorum

Estimate errors in R

```{r}
sampleGalton4 <- newGalton[sample(1:1e6,size=50,replace=F),] 
sampleLm4 <- lm(sampleGalton4$child ~ sampleGalton4$parent) 
summary(sampleLm4)
```

distribution of slopes

```{r}
hist(sapply(sampleLm,function(x){coef(x)[2]}),col="blue",xlab="Slope",main="",freq=F) 
lines(seq(0,5,length=100),dnorm(seq(0,5,length=100),mean=coef(sampleLm4)[2],
sd=summary(sampleLm4)$coeff[2,2]),lwd=3,col="red")
```

```{r}
par(mfrow=c(1,2)) 
hist(sapply(sampleLm,function(x){coef(x)[1]}),col="blue",xlab="Intercept",main="") 
hist(sapply(sampleLm,function(x){coef(x)[2]}),col="blue",xlab="Slope",main="")
```

t distributions

```{r}
x <- seq(-5,5,length=100) 
plot(x,dnorm(x),type="l",lwd=3) 
lines(x,dt(x,df=3),lwd=3,col="red") 
lines(x,dt(x,df=10),lwd=3,col="blue")
```

```{r}
summary(sampleLm4)$coeff
```

```{r}
confint(sampleLm4,level=0.95)
```

Confidence interval for each sample

```{r}
par(mar=c(4,4,0,2));plot(1:10,type="n",xlim=c(0,1.5),ylim=c(0,100), xlab="Coefficient Values",ylab="Replication")
for(i in 1:100){
  ci <- confint(sampleLm[[i]]); color="red";
  if((ci[2,1] < lm1$coeff[2]) & (lm1$coeff[2] < ci[2,2])){
    color = "grey"
  } 
  segments(ci[2,1],i,ci[2,2],i,col=color,lwd=3)
} 
lines(rep(lm1$coeff[2],100),seq(0,100,length=100),lwd=3)
```

Report the inference

```{r}
sampleLm4$coeff
```

```{r}
confint(sampleLm4,level=0.95)
```

P-Values
--------

```{r}
library(UsingR); data(galton) 
plot(galton$parent,galton$child,pch=19,col="blue") 
lm1 <- lm(galton$child ~ galton$parent) 
abline(lm1,col="red",lwd=3)
```

Null distribution

```{r}
x <- seq(-20,20,length=100) 
plot(x,dt(x,df=(928-2)),col="blue",lwd=3,type="l")
```

Null distribution + observed statistic

```{r}
x <- seq(-20,20,length=100) plot(x,dt(x,df=(928-2)),col="blue",lwd=3,type="l") 
arrows(summary(lm1)$coeff[2,3],0.25,summary(lm1)$coeff[2,3],0,col="red",lwd=4)
```

Calculating P-values

```{r}
summary(lm1)
```

Large p value because there is no relationship between x and y values:


```{r}
set.seed(9898324)
yValues <- rnorm(10); xValues <- rnorm(10) 
lm2 <- lm(yValues ~ xValues)
summary(lm2)
```

```{r}
x <- seq(-5,5,length=100)
plot(x,dt(x,df=(10-2)),col="blue",lwd=3,type="l") 
arrows(summary(lm2)$coeff[2,3],0.25,summary(lm2)$coeff[2,3],0,col="red",lwd=4)
```

```{r}
xCoords <- seq(-5,5,length=100) plot(xCoords,dt(xCoords,df=(10-2)),col="blue",lwd=3,type="l")
xSequence <- c(seq(summary(lm2)$coeff[2,3],5,length=10),summary(lm2)$coeff[2,3]) 
ySequence <- c(dt(seq(summary(lm2)$coeff[2,3],5,length=10),df=8),0) 
polygon(xSequence,ySequence,col="red"); polygon(-xSequence,ySequence,col="red")
```

Lots of data with no signal

```{r}
set.seed(8323); pValues <- rep(NA,100) 
for(i in 1:100){
  xValues <- rnorm(20);yValues <- rnorm(20)
  pValues[i] <- summary(lm(yValues ~ xValues))$coeff[2,4] 
}
hist(pValues,col="blue",main="",freq=F) 
abline(h=1,col="red",lwd=3)
```

Lots of data with a signal

```{r}
set.seed(8323); pValues <- rep(NA,100) 
for(i in 1:100){
  xValues <- rnorm(20);yValues <- 0.2 * xValues + rnorm(20)
  pValues[i] <- summary(lm(yValues ~ xValues))$coeff[2,4] 
}
hist(pValues,col="blue",main="",freq=F,xlim=c(0,1)); 
abline(h=1,col="red",lwd=3)
```


* P < 0.05 significant
* P < 0.01 strongly significant
* p < 0.001 very significant

```{r}
summary(lm(galton$child ~ galton$parent))$coeff
```

Regression with Factor Variables
--------------------------------

```{r}
movies <- read.csv("./data/movies.txt", sep="\t")
head(movies)
names(movies)
```

Score vs rating - jitter values to show stacked points

```{r}
plot(movies$score ~ jitter(as.numeric(movies$rating)),col="blue",xaxt="n",pch=19) 
axis(side=1,at=unique(as.numeric(movies$rating)),labels=unique(movies$rating))
```

Average score by rating

```{r}
plot(movies$score ~ jitter(as.numeric(movies$rating)),col="blue",xaxt="n",pch=19) 
axis(side=1,at=unique(as.numeric(movies$rating)),labels=unique(movies$rating)) 
meanRatings <- tapply(movies$score,movies$rating,mean) 
points(1:4,meanRatings,col="red",pch="-",cex=5)
```

```{r}
lm1 <- lm(movies$score ~ as.factor(movies$rating)) 
summary(lm1)
```



Real World
----------

Ideal data for regression:

```{r}
library(UsingR); 
data(galton) 
plot(galton$parent,galton$child,col="blue",pch=19)
```

points have cloud like oval shape

Confounders - a variable that is correlated with both the outcome and the covariates

```{r}
download.file("http://apps.who.int/gho/athena/data/GHO/WHOSIS_000008.csv?profile=text&filt er=COUNTRY:*;SEX:*","./data/hunger.csv",method="curl")
hunger <- read.csv("./data/hunger.csv") 
hunger <- hunger[hunger$Sex!="Both sexes",] 
head(hunger)
```

plot coloured by region - would get a totally different regression line for each region.

```{r}
par(mfrow=c(1,2))
plot(hunger$Year,hunger$Numeric,col=as.numeric(hunger$WHO.region),pch=19) 
plot(1:10,type="n",xaxt="n",yaxt="n",xlab="",ylab="") 
legend(1,10,col=unique(as.numeric(hunger$WHO.region)),legend=unique(hunger$WHO.region),pch=19)
```

region correlated with year

large f value and small pvalue suggest a strong correlation

```{r}
anova(lm(hunger$Year ~ hunger$WHO.region))
```

region correlated with hunger

```{r}
anova(lm(hunger$Numeric ~ hunger$WHO.region))
```

including region - a complicated interaction

```{r}
plot(hunger$Year,hunger$Numeric,pch=19,col=as.numeric(hunger$WHO.region))
lmRegion <- lm(hunger$Numeric ~ hunger$Year + hunger$WHO.region + hunger$Year*hunger$WHO.region ) 
abline(c(lmRegion$coeff[1] + lmRegion$coeff[6],lmRegion$coeff[2]+ lmRegion$coeff[12]),col=5,lwd=3)
```

income data

```{r}
download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data","./data/income.csv") 
incomeData <- read.csv("./data/income.csv",header=FALSE)
income <- incomeData[,3]
age <- incomeData[,1]
```

```{r}
par(mfrow=c(1,4)) 
smoothScatter(age,income) 
hist(income,col="blue",breaks=100) 
hist(log(income+1),col="blue",breaks=100) 
smoothScatter(age,log(income+1))
```



















